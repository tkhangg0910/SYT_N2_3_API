import yake
import streamlit as st
import article_crawl as ac
from flashtext import KeywordProcessor
from collections import Counter

DEFAULT_KEYWORDS = ["m·ªõi nh·∫•t", "hi·ªán ƒë·∫°i nh·∫•t", "ƒë·ªôc quy·ªÅn", "duy nh·∫•t", "ho√†n to√†n", "nh·∫•t", "ho√†n to√†n 100%"]

def main(): 
    # st.set_page_config(page_title="Demo L·ªçc T·ª´ kh√≥a Vi ph·∫°m", page_icon="üîç")
    st.title("Demo L·ªçc t·ª´ kh√≥a vi ph·∫°m t·ª´ url")

    url = st.text_input("Nh·∫≠p URL", placeholder="https://www.example.com")    
    filter_btn = st.button(label="L·ªçc", use_container_width=True)
    if filter_btn:
        show_result(url=url, keywords=DEFAULT_KEYWORDS)

def extract_keywords(article: str, keywords: list[str]) -> dict[str, int]:
    keyword_processor = KeywordProcessor()
    keyword_processor.add_keywords_from_list(keywords)
    extracted_keywords = keyword_processor.extract_keywords(article)
    
    # Count the frequency of each keyword
    keyword_frequency = dict(Counter(extracted_keywords))
    
    return keyword_frequency

def show_result(url:str, keywords: list[str]):
    # Ki·ªÉm tra n·∫øu URL v√† t·ª´ kh√≥a kh√¥ng ƒë∆∞·ª£c nh·∫≠p
    if not url:
        st.error("Vui l√≤ng nh·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin.")
    else:
        article = ac.crawl_and_clean_article(url=url)

        filtered_keywords = extract_keywords(article["content"], keywords)
        sentences_with_keywords = get_sentences_with_keywords(article, filtered_keywords.keys())

        if len(filtered_keywords) == 0:
            st.error("Kh√¥ng t√¨m th·∫•y t·ª´ kh√≥a trong b√†i vi·∫øt.")
        else:
            # st.subheader("Danh s√°ch c√¢u c√≥ ch·ª©a t·ª´ kh√≥a vi ph·∫°m")
            st.text_area("Danh s√°ch c√¢u c√≥ ch·ª©a t·ª´ kh√≥a vi ph·∫°m","- " + "\n- ".join([f"{s}" for s in sentences_with_keywords]), height=300)
            st.success("K·∫øt qu·∫£ l·ªçc <t·ª´ kh√≥a>: <s·ªë l·∫ßn xu·∫•t hi·ªán>:\n- " + "\n- ".join([f"{kw}: {f}" for kw, f in filtered_keywords.items()]))

        # st.success(f"T·ª´ kh√≥a kh√°c trong b√†i vi·∫øt - Yake: \n- " + "\n- ".join(topYake(article, DEFAULT_KEYWORDS.lower().split(", "), filtered=filtered_options, length=length_keywords_list)))

def get_sentences_with_keywords(article: ac._article, keywords: list[str]) -> list[str]:
    sentences = article["content"].split(". ")
    sentences_with_keywords = []
    for sentence in sentences:
        if any(keyword.lower() in sentence.lower() for keyword in keywords):
            sentences_with_keywords.append(sentence)
    return list(set(sentences_with_keywords))



# def topYake(article, word_list:list[str], length:int=10, filtered:bool=False):
#     kw_extractor = yake.KeywordExtractor(lan="vi", top=100)
#     article_keywords = kw_extractor.extract_keywords(article["content"] + ". " + article["title"])
#     sorted_article_keywords = sorted(article_keywords, key=lambda x: x[1], reverse=False)

#     if not filtered:
#         top = [kw for kw, score in sorted_article_keywords]
#         return top[:length]
#     filtered_keywords = [kw for kw, score in sorted_article_keywords if any(word.lower() in kw.lower() for word in word_list)]
#     return filtered_keywords[:length]

# def top10KeyBert(article):
#     kw_extractor = KeyBERT()
#     article_keywords = kw_extractor.extract_keywords((article["title"] + " " + article["content"]).lower(), keyphrase_ngram_range=(1, 3), stop_words='vietnamese', top_n=10)
#     top10 = [kw for kw, score in article_keywords]
#     return top10

main()
